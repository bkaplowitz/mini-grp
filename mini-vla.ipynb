{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c37713",
   "metadata": {},
   "source": [
    "# Using an LLM\n",
    "\n",
    "The base model so far has been untrained. How can we make use of prior models to accelerate the learning process and make use of the prior learned structure. Can we integrate an LLM? [OpenVLA](https://openvla.github.io/) is an example of this.\n",
    "\n",
    "Here is a example of this direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b840a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-02-06 16:16:54.901788: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-06 16:16:54.901837: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-06 16:16:54.903683: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-06 16:16:54.914331: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-06 16:16:57.470420: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2026-02-06 16:17:01.160210: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n",
      "2026-02-06 16:17:02.137321: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 31\n",
      "example text encode: [6, 18, 8, 10, 12, 0, 25, 15, 12, 0, 10, 8, 20, 0, 25, 21, 0, 25, 15, 12, 0, 18, 12, 13, 25, 0, 21, 13, 0, 25, 15, 12, 0, 22, 21, 25, 2]\n",
      "Dataset shape: 687\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Grab a chunk of data for training\n",
    "import tensorflow_datasets as tfds\n",
    "import cv2\n",
    "import numpy as np\n",
    "image_shape = [64, 64, 3]\n",
    "num_episodes = 20 ## How many episodes to grab from the dataset for training\n",
    "\n",
    "builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/bridge/0.1.0/')\n",
    "datasetRemote = builder.as_dataset(split='train[:' + str(num_episodes) + ']')\n",
    "dataset = {\"img\": [], \"action\": [], \"goal\": [], \"goal_img\": [],\n",
    "                \"rotation_delta\": [], \"open_gripper\": [] }\n",
    "shortest_goal_txt = 10000000000\n",
    "for episode in datasetRemote:\n",
    "    episode_ = {'steps': [] }\n",
    "    episode = list(episode['steps'])\n",
    "    ## Goal image is just the last image/state/observation in the episode\n",
    "    goal_img = cv2.resize(np.array(episode[-1]['observation']['image'], dtype=np.float32), (image_shape[0], image_shape[1]))\n",
    "    for i in range(len(episode)):\n",
    "        obs = cv2.resize(np.array(episode[i]['observation']['image'], dtype=np.float32), (image_shape[0], image_shape[1]))\n",
    "        goal = episode[i]['observation']['natural_language_instruction'].numpy().decode()\n",
    "        dataset[\"img\"].append(obs)\n",
    "        dataset[\"action\"].append(np.array(np.concatenate((episode[i]['action']['world_vector'], \n",
    "                                                          episode[i]['action']['rotation_delta'],\n",
    "                                                        [episode[i]['action']['open_gripper']]), axis=0)))\n",
    "         \n",
    "        dataset[\"rotation_delta\"].append(np.array(episode[i]['action']['rotation_delta']))\n",
    "        dataset[\"open_gripper\"].append(np.array(episode[i]['action']['open_gripper']))\n",
    "        dataset[\"goal\"].append(goal)\n",
    "        dataset[\"goal_img\"].append(goal_img)\n",
    "        if len(goal) < shortest_goal_txt: shortest_goal_txt = len(goal)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set([item for row in dataset[\"goal\"] for item in row])))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode_txt = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode_txy = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "print(\"example text encode:\", encode_txt(dataset[\"goal\"][0]))\n",
    "\n",
    "print(\"Dataset shape:\", len(dataset[\"img\"]))\n",
    "dataset[\"img\"] = np.array(dataset[\"img\"], dtype=np.uint8)\n",
    "dataset[\"action\"] = np.array(dataset[\"action\"], dtype=np.float32)\n",
    "# dataset[\"goal\"] = np.array(encode_txt(dataset[\"goal\"]), dtype=np.float32)\n",
    "dataset[\"goal_img\"] = np.array(dataset[\"goal_img\"], dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3626f36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "## This is an encoder head (full attention)\n",
    "print (\"torch version:\", torch.__version__)\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B,T,C = x.shape\n",
    "        if mask == None:\n",
    "            mask = torch.ones((T, ), device=x.device) ## (1, T)\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        ### Block masked attention\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd=n_embd, dropout=dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = torch.cat([h(x, mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x,)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd=n_embd, dropout=dropout)\n",
    "        self.ffwd = FeedFoward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.sa(self.ln1(x), mask)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GRP(nn.Module):\n",
    "  def __init__(self, dataset, cfg, mlp_ratio=4):\n",
    "    super(GRP, self).__init__()\n",
    "    self._dataset = dataset\n",
    "    self._cfg = cfg\n",
    "\n",
    "    self.token_embedding_table = nn.Embedding(cfg.vocab_size, cfg.n_embd)\n",
    "    self.patch_size = (self._cfg.image_shape[0] / self._cfg.n_patches, self._cfg.image_shape[1] / self._cfg.n_patches)\n",
    "\n",
    "    #Positional embedding\n",
    "    self.register_buffer('positional_embeddings', calc_positional_embeddings(1 + self._cfg.n_patches ** 2 + self._cfg.block_size + self._cfg.n_patches ** 2, cfg.n_embd), persistent=False)\n",
    "    \n",
    "    self.class_tokens = nn.Parameter(torch.rand(1, cfg.n_embd))\n",
    "\n",
    "    self.input_d = int(self._cfg.image_shape[2] * self.patch_size[0] * self.patch_size[1])\n",
    "\n",
    "    self.lin_map = nn.Linear(self.input_d, self._cfg.n_embd, bias=False) \n",
    "\n",
    "    # 4) Transformer encoder blocks\n",
    "    self.blocks = nn.ModuleList([Block(self._cfg.n_embd, self._cfg.n_head, dropout=self._cfg.dropout) for _ in range(self._cfg.n_blocks)])\n",
    "\n",
    "    # 5) Classification MLPk\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(self._cfg.n_embd, self._cfg.action_bins),\n",
    "        # nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def _init_weights(self, module):\n",
    "      if isinstance(module, nn.Linear):\n",
    "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "          if module.bias is not None:\n",
    "              torch.nn.init.zeros_(module.bias)\n",
    "      elif isinstance(module, nn.Embedding):\n",
    "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "  def forward(self, images, goals, goal_imgs, targets=None):\n",
    "    # Dividing images into patches\n",
    "    n, c, h, w = images.shape\n",
    "    B, T = goals.shape\n",
    "    patches = get_patches_fast(images)\n",
    "    patches_g = get_patches_fast(goal_imgs)\n",
    "    goals_e = self.token_embedding_table(goals)\n",
    "    \n",
    "    # Running linear layer tokenization\n",
    "    # Map the vector corresponding to each patch to the hidden size dimension\n",
    "    out = self.lin_map(patches)\n",
    "    out_g = self.lin_map(patches_g)\n",
    "    \n",
    "    # Adding classification and goal_img tokens to the tokens\n",
    "    out = torch.cat((out, goals_e, out_g, self.class_tokens.expand(n, 1, -1)), dim=1)\n",
    "    \n",
    "    # Adding positional embedding\n",
    "    out = out + self.positional_embeddings.repeat(n, 1, 1)\n",
    "\n",
    "    ## Compute blocked masks\n",
    "    mask = torch.ones((c + T + c + 1), device=self._cfg.device) ## (1, T)\n",
    "    if targets is None:\n",
    "        pass\n",
    "    elif (torch.rand(1)[0] > 0.66):  \n",
    "        mask[c: c+ T] = torch.zeros((1,T), device=self._cfg.device) ## Mask goal string\n",
    "    elif (torch.rand(1)[0] > 0.33):\n",
    "        mask[c + T: c + T + c] = torch.zeros((1,c), device=self._cfg.device) ## Mask goal image\n",
    "        \n",
    "    # Transformer Blocks\n",
    "    for block in self.blocks:\n",
    "        out = block(out, mask)\n",
    "\n",
    "    # Getting the classification token only\n",
    "    out = out[:, -1]\n",
    "    out = self.mlp(out)\n",
    "        \n",
    "    if targets is None:\n",
    "        loss = None\n",
    "    else:\n",
    "        B, C = out.shape\n",
    "        loss = F.mse_loss(out, targets) ## B, C\n",
    "    return (out, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36568548",
   "metadata": {},
   "source": [
    "## Vision Language Action (VLA) Model\n",
    "\n",
    "This section implements a VLA model that combines:\n",
    "- **T5 Text Encoder**: For processing natural language instructions\n",
    "- **CNN Vision Encoder**: For processing current and goal images\n",
    "- **Pretrained LLM**: For action prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65644953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers library imported successfully\n",
      "transformers version: 4.41.0\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries for VLA model\n",
    "# !pip install transformers==4.41.0\n",
    "from transformers import T5Tokenizer, T5EncoderModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "print(\"Transformers library imported successfully\")\n",
    "print(\"transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a587d293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Image Encoder defined\n"
     ]
    }
   ],
   "source": [
    "## CNN Image Encoder for processing visual observations\n",
    "class CNNImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network for encoding images into feature vectors.\n",
    "    Processes both current observation and goal images.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_shape=[64, 64, 3], output_dim=512):\n",
    "        super(CNNImageEncoder, self).__init__()\n",
    "        self.image_shape = image_shape\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)  # 64x64 -> 32x32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1) # 32x32 -> 16x16\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1) # 16x16 -> 8x8\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1) # 8x8 -> 4x4\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Calculate flattened size: 256 channels * 4 * 4 = 4096\n",
    "        self.fc = nn.Linear(256 * 4 * 4, output_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input: (B, C, H, W) where C=3, H=64, W=64\n",
    "        # Expect input in range [-1, 1]\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        # Flatten with reshape\n",
    "        x = x.reshape(x.size(0), -1)  # (B, 256*4*4)\n",
    "        #\n",
    "        # x = x.view(x.size(0), -1)  # (B, 256*4*4)\n",
    "        \n",
    "        # Project to output dimension\n",
    "        x = self.dropout(F.relu(self.fc(x)))\n",
    "        \n",
    "        return x  # (B, output_dim)\n",
    "\n",
    "print(\"CNN Image Encoder defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b3d0fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLA Model defined successfully\n"
     ]
    }
   ],
   "source": [
    "## Vision Language Action (VLA) Model\n",
    "class VLAModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Language Action Model that combines:\n",
    "    - T5 text encoder for processing language instructions\n",
    "    - CNN vision encoder for processing current and goal images\n",
    "    - Pretrained LLM backbone for multimodal fusion and action prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, t5_model_name='google/t5-v1_1-small', llm_hidden_dim=512):\n",
    "        super(VLAModel, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.llm_hidden_dim = llm_hidden_dim\n",
    "        \n",
    "        # 1. T5 Text Encoder (frozen by default, can be fine-tuned)\n",
    "        print(f\"Loading T5 model: {t5_model_name}\")\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "        self.text_encoder = T5EncoderModel.from_pretrained(t5_model_name)\n",
    "        self.t5_hidden_dim = self.text_encoder.config.d_model\n",
    "        \n",
    "        # Option to freeze T5 parameters\n",
    "        if cfg.get('freeze_text_encoder', True):\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"T5 text encoder frozen\")\n",
    "        \n",
    "        # 2. CNN Vision Encoder\n",
    "        self.vision_encoder = CNNImageEncoder(\n",
    "            image_shape=cfg.image_shape,\n",
    "            output_dim=llm_hidden_dim\n",
    "        )\n",
    "        \n",
    "        # 3. Projection layers to align dimensions\n",
    "        self.text_projection = nn.Linear(self.t5_hidden_dim, llm_hidden_dim)\n",
    "        self.vision_projection = nn.Linear(llm_hidden_dim, llm_hidden_dim)\n",
    "        \n",
    "        # 4. Cross-modal fusion transformer\n",
    "        self.fusion_layers = nn.ModuleList([\n",
    "            Block(llm_hidden_dim, n_head=cfg.n_head, dropout=cfg.dropout)\n",
    "            for _ in range(cfg.get('n_fusion_blocks', 4))\n",
    "        ])\n",
    "        \n",
    "        # 5. Action prediction head\n",
    "        self.action_head = nn.Sequential(\n",
    "            nn.Linear(llm_hidden_dim, llm_hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.Linear(llm_hidden_dim * 2, cfg.action_bins)\n",
    "        )\n",
    "        \n",
    "        # Learnable aggregation token for pooling multimodal features\n",
    "        self.aggregation_token = nn.Parameter(torch.randn(1, 1, llm_hidden_dim))\n",
    "        \n",
    "    def encode_text(self, text_instructions):\n",
    "        \"\"\"\n",
    "        Encode text instructions using T5 encoder\n",
    "        Args:\n",
    "            text_instructions: List of text strings\n",
    "        Returns:\n",
    "            text_embeddings: (B, T, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Tokenize text\n",
    "        encoded = self.tokenizer(\n",
    "            text_instructions,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.cfg.get('max_text_length', 64),\n",
    "            return_tensors='pt'\n",
    "        ).to(next(self.text_encoder.parameters()).device)\n",
    "        \n",
    "        # Get T5 embeddings\n",
    "        with torch.no_grad() if self.cfg.get('freeze_text_encoder', True) else torch.enable_grad():\n",
    "            outputs = self.text_encoder(**encoded)\n",
    "            text_embeddings = outputs.last_hidden_state  # (B, seq_len, d_model)\n",
    "        \n",
    "        # Project to LLM hidden dimension\n",
    "        text_embeddings = self.text_projection(text_embeddings)  # (B, seq_len, llm_hidden_dim)\n",
    "        \n",
    "        return text_embeddings\n",
    "    \n",
    "    def encode_vision(self, current_obs, goal_obs):\n",
    "        \"\"\"\n",
    "        Encode current and goal images using CNN\n",
    "        Args:\n",
    "            current_obs: (B, C, H, W) current observation images\n",
    "            goal_obs: (B, C, H, W) goal images\n",
    "        Returns:\n",
    "            vision_embeddings: (B, 2, hidden_dim) - 2 tokens for current and goal\n",
    "        \"\"\"\n",
    "        # Encode both images\n",
    "        current_features = self.vision_encoder(current_obs)  # (B, hidden_dim)\n",
    "        goal_features = self.vision_encoder(goal_obs)  # (B, hidden_dim)\n",
    "        \n",
    "        # Project features\n",
    "        current_features = self.vision_projection(current_features)\n",
    "        goal_features = self.vision_projection(goal_features)\n",
    "        \n",
    "        # Stack to create sequence: [current, goal]\n",
    "        vision_embeddings = torch.stack([current_features, goal_features], dim=1)  # (B, 2, hidden_dim)\n",
    "        \n",
    "        return vision_embeddings\n",
    "    \n",
    "    def forward(self, images, goal_texts, goal_images, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args:\n",
    "            images: (B, C, H, W) current observation images (in [-1, 1])\n",
    "            goal_texts: List of B text strings describing the goal\n",
    "            goal_images: (B, C, H, W) goal images (in [-1, 1])\n",
    "            targets: (B, action_dim) target actions for training\n",
    "        Returns:\n",
    "            action_predictions: (B, action_dim)\n",
    "            loss: scalar loss if targets provided\n",
    "        \"\"\"\n",
    "        batch_size = images.shape[0]\n",
    "        device = images.device\n",
    "        \n",
    "        # 1. Encode text instructions\n",
    "        text_embeddings = self.encode_text(goal_texts)  # (B, T_text, hidden_dim)\n",
    "        \n",
    "        # 2. Encode vision (current + goal images)\n",
    "        ## Need to update image to channel first\n",
    "        # images = images.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "        # goal_images = goal_images.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "        vision_embeddings = self.encode_vision(images, goal_images)  # (B, 2, hidden_dim)\n",
    "        \n",
    "        # 3. Concatenate all tokens: [text_tokens, vision_tokens, aggregation_token]\n",
    "        agg_token = self.aggregation_token.expand(batch_size, -1, -1)  # (B, 1, hidden_dim)\n",
    "        multimodal_tokens = torch.cat([\n",
    "            text_embeddings,\n",
    "            vision_embeddings,\n",
    "            agg_token\n",
    "        ], dim=1)  # (B, T_text + 2 + 1, hidden_dim)\n",
    "        \n",
    "        # 4. Apply fusion transformer layers\n",
    "        fused_features = multimodal_tokens\n",
    "        for fusion_layer in self.fusion_layers:\n",
    "            fused_features = fusion_layer(fused_features)\n",
    "        \n",
    "        # 5. Extract aggregation token for action prediction\n",
    "        action_features = fused_features[:, -1, :]  # (B, hidden_dim)\n",
    "        \n",
    "        # 6. Predict actions\n",
    "        action_predictions = self.action_head(action_features)  # (B, action_dim)\n",
    "        \n",
    "        # 7. Compute loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.mse_loss(action_predictions, targets)\n",
    "        \n",
    "        return action_predictions, loss\n",
    "\n",
    "print(\"VLA Model defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6145e2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLA training utilities defined\n"
     ]
    }
   ],
   "source": [
    "## Helper function to use VLA model instead of GRP\n",
    "def create_vla_model(cfg, device='cuda'):\n",
    "    \"\"\"\n",
    "    Create and initialize VLA model to replace GRP\n",
    "    \"\"\"\n",
    "    # Add VLA-specific config if not present\n",
    "    if not hasattr(cfg, 'freeze_text_encoder'):\n",
    "        cfg.freeze_text_encoder = True  # Freeze T5 by default\n",
    "    if not hasattr(cfg, 'n_fusion_blocks'):\n",
    "        cfg.n_fusion_blocks = 4  # Number of fusion transformer blocks\n",
    "    if not hasattr(cfg, 'max_text_length'):\n",
    "        cfg.max_text_length = 64  # Max text sequence length\n",
    "    \n",
    "    # Create VLA model\n",
    "    model = VLAModel(\n",
    "        cfg=cfg,\n",
    "        t5_model_name='google/t5-v1_1-small',  # Can change to 'base' or 'large'\n",
    "        llm_hidden_dim=512\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params/1e6:.2f}M\")\n",
    "    print(f\"Trainable parameters: {trainable_params/1e6:.2f}M\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "## Modified training function for VLA\n",
    "def train_vla_model(model, dataset, cfg, device='cuda'):\n",
    "    \"\"\"\n",
    "    Training loop for VLA model\n",
    "    Note: VLA expects text strings, not encoded characters\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=float(cfg.learning_rate)\n",
    "    )\n",
    "    \n",
    "    for iter in range(cfg.max_iters):\n",
    "        # Evaluate loss periodically\n",
    "        if iter % cfg.eval_interval == 0 or iter == cfg.max_iters - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Sample validation batch\n",
    "                indices = np.random.choice(len(dataset[\"img\"]), cfg.batch_size, replace=False)\n",
    "                val_images = dataset[\"image_enc\"][indices].permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "                val_goal_texts = [dataset[\"goal\"][i] for i in indices]\n",
    "                val_goal_images = dataset[\"goal_image_enc\"][indices].permute(0, 3, 1, 2)\n",
    "                val_actions = dataset[\"action_enc\"][indices]\n",
    "                \n",
    "                _, val_loss = model(val_images, val_goal_texts, val_goal_images, val_actions)\n",
    "                print(f\"step {iter}: val loss {val_loss.item():.4f}\")\n",
    "            model.train()\n",
    "        \n",
    "        # Sample training batch\n",
    "        indices = np.random.choice(len(dataset[\"img\"]), cfg.batch_size, replace=False)\n",
    "        images = dataset[\"image_enc\"][indices].permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "        goal_texts = [dataset[\"goal\"][i] for i in indices]  # List of strings\n",
    "        goal_images = dataset[\"goal_image_enc\"][indices].permute(0, 3, 1, 2)\n",
    "        actions = dataset[\"action_enc\"][indices]\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions, loss = model(images, goal_texts, goal_images, actions)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iter % 100 == 0:\n",
    "            print(f\"step {iter}: train loss {loss.item():.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"VLA training utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1982cdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading T5 model: google/t5-v1_1-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 text encoder frozen\n",
      "Total parameters: 51.48M\n",
      "Trainable parameters: 16.15M\n",
      "Test batch shapes:\n",
      "  Images: torch.Size([4, 3, 64, 64])\n",
      "  Goal texts: 4 strings\n",
      "  Goal images: torch.Size([4, 3, 64, 64])\n",
      "  Actions: torch.Size([4, 7])\n",
      "\\nVLA Model forward pass successful!\n",
      "  Predictions shape: torch.Size([4, 7])\n",
      "  Loss: 0.2945\n",
      "step 0: val loss 0.6108\n",
      "step 0: train loss 0.4853\n",
      "step 100: val loss 0.4716\n",
      "step 100: train loss 0.3479\n",
      "step 200: val loss 0.2215\n",
      "step 200: train loss 0.1673\n",
      "step 300: val loss 0.1098\n",
      "step 300: train loss 0.1197\n",
      "step 400: val loss 0.0907\n",
      "step 400: train loss 0.0824\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3222185/551402741.py\", line 50, in <module>\n",
      "    vla_model = train_vla_model(vla_model, dataset, cfg, device=device)\n",
      "  File \"/tmp/ipykernel_3222185/1077614655.py\", line 70, in train_vla_model\n",
      "    loss.backward()\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2168, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1457, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1348, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1195, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1110, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 992, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 804, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "## Example: Create and test VLA model\n",
    "## Uncomment to run VLA instead of GRP\n",
    "\n",
    "\n",
    "# Create VLA model\n",
    "from box import Box\n",
    "import yaml\n",
    "\n",
    "# Load config\n",
    "with open('./conf/config.yaml', 'r') as f:\n",
    "    cfg_dict = yaml.safe_load(f)    \n",
    "cfg = Box(cfg_dict)\n",
    "\n",
    "# Prepare data encodings\n",
    "a_std, a_mean = (dataset[\"action\"].std(axis=0) + 0.001) * 1.5, dataset[\"action\"].mean(axis=0)\n",
    "cfg.action_bins = len(a_mean)\n",
    "encode_action = lambda af: (((af - a_mean)/(a_std))).astype(np.float32)\n",
    "encode_state = lambda af: ((af/(255.0)*2.0)-1.0).astype(np.float32)\n",
    "\n",
    "# Encode dataset for VLA\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dataset[\"image_enc\"] = torch.tensor(encode_state(dataset[\"img\"])).to(device)\n",
    "dataset[\"goal_image_enc\"] = torch.tensor(encode_state(dataset[\"goal_img\"])).to(device)\n",
    "dataset[\"action_enc\"] = torch.tensor(encode_action(dataset[\"action\"]), dtype=torch.float).to(device)\n",
    "\n",
    "# Initialize VLA model\n",
    "vla_model = create_vla_model(cfg, device=device)\n",
    "\n",
    "# Test forward pass with a small batch\n",
    "test_batch_size = 4\n",
    "test_indices = np.random.choice(len(dataset[\"img\"]), test_batch_size, replace=False)\n",
    "test_images = dataset[\"image_enc\"][test_indices].permute(0, 3, 1, 2)\n",
    "test_goal_texts = [dataset[\"goal\"][i] for i in test_indices]\n",
    "test_goal_images = dataset[\"goal_image_enc\"][test_indices].permute(0, 3, 1, 2)\n",
    "test_actions = dataset[\"action_enc\"][test_indices]\n",
    "\n",
    "print(f\"Test batch shapes:\")\n",
    "print(f\"  Images: {test_images.shape}\")\n",
    "print(f\"  Goal texts: {len(test_goal_texts)} strings\")\n",
    "print(f\"  Goal images: {test_goal_images.shape}\")\n",
    "print(f\"  Actions: {test_actions.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "predictions, loss = vla_model(test_images, test_goal_texts, test_goal_images, test_actions)\n",
    "print(f\"\\\\nVLA Model forward pass successful!\")\n",
    "print(f\"  Predictions shape: {predictions.shape}\")\n",
    "print(f\"  Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Train VLA model (uncomment to train)\n",
    "vla_model = train_vla_model(vla_model, dataset, cfg, device=device)\n",
    "\n",
    "\n",
    "print(\"VLA example code ready (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc2969",
   "metadata": {},
   "source": [
    "## Key Differences: VLA vs GRP\n",
    "\n",
    "### VLA Model Architecture:\n",
    "1. **Text Encoding**: Uses pretrained T5 encoder instead of character-level embeddings\n",
    "   - Better semantic understanding of instructions\n",
    "   - Transfer learning from T5's pretraining\n",
    "   - Can be frozen to save compute\n",
    "\n",
    "2. **Vision Encoding**: Uses CNN instead of patch-based ViT approach\n",
    "   - More parameter efficient\n",
    "   - Better for smaller image sizes (64x64)\n",
    "   - Separate encoding of current and goal images\n",
    "\n",
    "3. **Multimodal Fusion**: Uses cross-attention transformer blocks\n",
    "   - Learns interactions between vision and language\n",
    "   - Aggregation token for pooling multimodal information\n",
    "   - More flexible than simple concatenation\n",
    "\n",
    "4. **Action Prediction**: MLP head on fused features\n",
    "   - Direct regression to continuous actions\n",
    "   - No vocabulary binning needed\n",
    "\n",
    "### Advantages:\n",
    "- ✅ Leverages pretrained language models (T5)\n",
    "- ✅ More parameter efficient with frozen encoders\n",
    "- ✅ Better language understanding\n",
    "- ✅ Modular design (easy to swap components)\n",
    "- ✅ Can handle variable-length text naturally\n",
    "\n",
    "### Usage:\n",
    "Replace `model = GRP(dataset, cfg)` with `model = create_vla_model(cfg)` in the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a164e2",
   "metadata": {},
   "source": [
    "# A VLA from a VLM\n",
    "\n",
    "Finally, recent models are basing their structures off of Vision Language Models.\n",
    "\n",
    "- The challenges with using an LLM and having to decode the action space and figure out how to put in the images and which token in the language space to replace with the new image tokens. These don't scale well.\n",
    "\n",
    "An example of [Pi0](https://www.pi.website/blog/pi0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e3656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaliGemma VLA utilities defined\n"
     ]
    }
   ],
   "source": [
    "## PaliGemma3B-based VLA model (image + goal image + text -> continuous actions)\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "class PaliGemmaVLA(nn.Module):\n",
    "    \"\"\"\n",
    "    VLA model using PaliGemma backbone with a separate action head.\n",
    "    It fuses text + concatenated current/goal images, then predicts continuous actions.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, model_name='google/paligemma-3b-pt-224', freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        self.backbone = PaliGemmaForConditionalGeneration.from_pretrained(model_name)\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "        self.action_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.Linear(hidden_size * 2, cfg.action_bins),\n",
    "        )\n",
    "\n",
    "    def _prepare_images(self, images, goal_images):\n",
    "        # Convert [-1, 1] float tensors to uint8 HWC and concatenate along width.\n",
    "        images_u8 = ((images + 1.0) * 127.5).clamp(0, 255).to(torch.uint8)\n",
    "        goals_u8 = ((goal_images + 1.0) * 127.5).clamp(0, 255).to(torch.uint8)\n",
    "        images_u8 = images_u8.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        goals_u8 = goals_u8.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        combined = [np.concatenate([img, goal], axis=1) for img, goal in zip(images_u8, goals_u8)]\n",
    "        return combined\n",
    "\n",
    "    def forward(self, images, goal_texts, goal_images, targets=None):\n",
    "        combined_images = self._prepare_images(images, goal_images)\n",
    "        inputs = self.processor(\n",
    "            text=goal_texts,\n",
    "            images=combined_images,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "        ).to(self.backbone.device)\n",
    "        outputs = self.backbone(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        last_hidden = outputs.hidden_states[-1]  # (B, seq_len, hidden)\n",
    "        pooled = last_hidden[:, -1, :]\n",
    "        action_predictions = self.action_head(pooled)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.mse_loss(action_predictions, targets)\n",
    "        return action_predictions, loss\n",
    "\n",
    "def create_paligemma_vla_model(cfg, device='cuda', freeze_backbone=True):\n",
    "    if not hasattr(cfg, 'max_text_length'):\n",
    "        cfg.max_text_length = 64\n",
    "    model = PaliGemmaVLA(\n",
    "        cfg=cfg,\n",
    "        model_name='google/paligemma-3b-pt-224',\n",
    "        freeze_backbone=freeze_backbone,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params/1e6:.2f}M\")\n",
    "    print(f\"Trainable parameters: {trainable_params/1e6:.2f}M\")\n",
    "    return model\n",
    "\n",
    "def train_paligemma_vla_model(model, dataset, cfg, device='cuda'):\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=float(cfg.learning_rate),\n",
    "    )\n",
    "    for iter in range(cfg.max_iters):\n",
    "        if iter % cfg.eval_interval == 0 or iter == cfg.max_iters - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                indices = np.random.choice(len(dataset[\"img\"]), cfg.batch_size, replace=False)\n",
    "                val_images = dataset[\"image_enc\"][indices].permute(0, 3, 1, 2)\n",
    "                val_goal_texts = [dataset[\"goal\"][i] for i in indices]\n",
    "                val_goal_images = dataset[\"goal_image_enc\"][indices].permute(0, 3, 1, 2)\n",
    "                val_actions = dataset[\"action_enc\"][indices]\n",
    "                _, val_loss = model(val_images, val_goal_texts, val_goal_images, val_actions)\n",
    "                print(f\"step {iter}: val loss {val_loss.item():.4f}\")\n",
    "            model.train()\n",
    "        indices = np.random.choice(len(dataset[\"img\"]), cfg.batch_size, replace=False)\n",
    "        images = dataset[\"image_enc\"][indices].permute(0, 3, 1, 2)\n",
    "        goal_texts = [dataset[\"goal\"][i] for i in indices]\n",
    "        goal_images = dataset[\"goal_image_enc\"][indices].permute(0, 3, 1, 2)\n",
    "        actions = dataset[\"action_enc\"][indices]\n",
    "        predictions, loss = model(images, goal_texts, goal_images, actions)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if iter % 100 == 0:\n",
    "            print(f\"step {iter}: train loss {loss.item():.4f}\")\n",
    "    return model\n",
    "\n",
    "print(\"PaliGemma VLA utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4e983c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]\n",
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2931.89M\n",
      "Trainable parameters: 8.42M\n",
      "Test batch shapes:\n",
      "  Images: torch.Size([4, 3, 64, 64])\n",
      "  Goal texts: 4 strings\n",
      "  Goal images: torch.Size([4, 3, 64, 64])\n",
      "  Actions: torch.Size([4, 7])\n",
      "  VLA Model forward pass successful!\n",
      "  Predictions shape: torch.Size([4, 7])\n",
      "  Loss: 0.7738\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1859699/988130187.py\", line 50, in <module>\n",
      "    vla_model = train_paligemma_vla_model(vla_model, dataset, cfg, device=device)\n",
      "  File \"/tmp/ipykernel_1859699/3479941056.py\", line 84, in train_paligemma_vla_model\n",
      "    _, val_loss = model(val_images, val_goal_texts, val_goal_images, val_actions)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1859699/3479941056.py\", line 46, in forward\n",
      "    outputs = self.backbone(**inputs, output_hidden_states=True, return_dict=True)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/transformers/models/paligemma/modeling_paligemma.py\", line 437, in forward\n",
      "    outputs = self.language_model(\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py\", line 1098, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py\", line 902, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py\", line 652, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py\", line 185, in forward\n",
      "    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.11 GiB. GPU 0 has a total capacity of 31.73 GiB of which 516.50 MiB is free. Including non-PyTorch memory, this process has 31.22 GiB memory in use. Of the allocated memory 29.05 GiB is allocated by PyTorch, and 1.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2168, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1457, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1348, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1195, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1110, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 992, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 804, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "  File \"/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/stack_data/core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "## Example: Create and test VLA model\n",
    "## Uncomment to run VLA instead of GRP\n",
    "\n",
    "\n",
    "# Create VLA model\n",
    "from box import Box\n",
    "import yaml\n",
    "\n",
    "# Load config\n",
    "with open('./conf/config.yaml', 'r') as f:\n",
    "    cfg_dict = yaml.safe_load(f)    \n",
    "cfg = Box(cfg_dict)\n",
    "\n",
    "# Prepare data encodings\n",
    "a_std, a_mean = (dataset[\"action\"].std(axis=0) + 0.001) * 1.5, dataset[\"action\"].mean(axis=0)\n",
    "cfg.action_bins = len(a_mean)\n",
    "encode_action = lambda af: (((af - a_mean)/(a_std))).astype(np.float32)\n",
    "encode_state = lambda af: ((af/(255.0)*2.0)-1.0).astype(np.float32)\n",
    "\n",
    "# Encode dataset for VLA\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dataset[\"image_enc\"] = torch.tensor(encode_state(dataset[\"img\"])).to(device)\n",
    "dataset[\"goal_image_enc\"] = torch.tensor(encode_state(dataset[\"goal_img\"])).to(device)\n",
    "dataset[\"action_enc\"] = torch.tensor(encode_action(dataset[\"action\"]), dtype=torch.float).to(device)\n",
    "\n",
    "# Initialize VLA model\n",
    "vla_model = create_paligemma_vla_model(cfg, device=device)\n",
    "\n",
    "# Test forward pass with a small batch\n",
    "test_batch_size = 4\n",
    "test_indices = np.random.choice(len(dataset[\"img\"]), test_batch_size, replace=False)\n",
    "test_images = dataset[\"image_enc\"][test_indices].permute(0, 3, 1, 2)\n",
    "test_goal_texts = [dataset[\"goal\"][i] for i in test_indices]\n",
    "test_goal_images = dataset[\"goal_image_enc\"][test_indices].permute(0, 3, 1, 2)\n",
    "test_actions = dataset[\"action_enc\"][test_indices]\n",
    "\n",
    "print(f\"Test batch shapes:\")\n",
    "print(f\"  Images: {test_images.shape}\")\n",
    "print(f\"  Goal texts: {len(test_goal_texts)} strings\")\n",
    "print(f\"  Goal images: {test_goal_images.shape}\")\n",
    "print(f\"  Actions: {test_actions.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "predictions, loss = vla_model(test_images, test_goal_texts, test_goal_images, test_actions)\n",
    "print(f\"  VLA Model forward pass successful!\")\n",
    "print(f\"  Predictions shape: {predictions.shape}\")\n",
    "print(f\"  Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Train VLA model (uncomment to train)\n",
    "vla_model = train_paligemma_vla_model(vla_model, dataset, cfg, device=device)\n",
    "\n",
    "\n",
    "print(\"VLA example code ready (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-boximport hydra, json\n",
    "\n",
    "def my_main():\n",
    "    from hydra import compose, initialize\n",
    "    # initialize(config_path=\"./conf\", job_name=\"test_app\")\n",
    "    from box import Box\n",
    "    import yaml\n",
    "    with open('./conf/config.yaml', 'r') as f:\n",
    "        cfg_dict = yaml.safe_load(f)    \n",
    "    cfg = Box(cfg_dict)\n",
    "    torch.manual_seed(cfg.r_seed)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "    cfg.block_size = shortest_goal_txt = min([len(txt) for txt in dataset[\"goal\"]])\n",
    "\n",
    "    # here are all the unique characters that occur in this text\n",
    "    chars = sorted(list(set([item for row in dataset[\"goal\"] for item in row]))) ## Flatten to a long string\n",
    "    cfg.vocab_size = len(chars)\n",
    "    # create a mapping from characters to integers\n",
    "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "    itos = { i:ch for i,ch in enumerate(chars) }\n",
    "    encode_txt = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "    decode_txy = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "    print(\"vocab_size:\", cfg.vocab_size)\n",
    "    print(\"example text encode:\", encode_txt(dataset[\"goal\"][0]))\n",
    "\n",
    "    a_std, a_mean = (dataset[\"action\"].std(axis=0) + 0.001) * 1.5, dataset[\"action\"].mean(axis=0)\n",
    "    cfg.action_bins = len(a_mean)\n",
    "    encode_action = lambda af:   (((af - a_mean)/(a_std))).astype(np.float32) # encoder: take a float, output an integer\n",
    "\n",
    "    ## Get the actions and encode them to map to [-1, 1]\n",
    "    encode_state = lambda af:   ((af/(255.0)*2.0)-1.0).astype(np.float32) # encoder: take a float, output an integer\n",
    "    resize_state = lambda sf:   cv2.resize(np.array(sf, dtype=np.float32), (cfg.image_shape[0], cfg.image_shape[1]))  # resize state\n",
    "    decode_action = lambda binN: (binN * a_std) + a_mean  # Undo mapping to [-1, 1]\n",
    "\n",
    "    dataset[\"goal_enc\"] = torch.tensor([encode_txt(goal[:cfg.block_size]) for goal in dataset[\"goal\"]]).to(device)\n",
    "    dataset[\"image_enc\"] = torch.tensor(encode_state(dataset[\"img\"])).to(device)\n",
    "    dataset[\"goal_image_enc\"] = torch.tensor(encode_state(dataset[\"goal_img\"])).to(device)\n",
    "    dataset[\"action_enc\"] = torch.tensor(encode_action(dataset[\"action\"]), dtype=torch.float).to(device)\n",
    "\n",
    "    print(\"Dataset shape:\", len(dataset[\"img\"]))\n",
    "\n",
    "    model = GRP(dataset, cfg)\n",
    "    m = model.to(device)\n",
    "    # print the number of parameters in the model\n",
    "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=float(cfg.learning_rate))\n",
    "\n",
    "    for iter in range(cfg.max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % cfg.eval_interval == 0 or iter == cfg.max_iters - 1:\n",
    "            losses = estimate_loss(model)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, xg, xgi, yb = get_batch_grp('train', dataset, cfg.batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, xg, xgi, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = my_main()\n",
    "    print(\"results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
